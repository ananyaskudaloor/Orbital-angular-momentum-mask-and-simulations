{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "EenS6uVOyE5k"
      },
      "outputs": [],
      "source": [
        "#OAM Diffraction order analysis\n",
        "!pip -q install opencv-python-headless numpy matplotlib scipy\n",
        "\n",
        "import numpy as np, cv2, matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "from collections import defaultdict\n",
        "import os\n",
        "\n",
        "#geometrical and parameters to tweak\n",
        "USE_RED_IF_COLOR      = True\n",
        "CENTER_CROP_TO_COMMON = True\n",
        "AVERAGING_METHOD      = \"mean\"\n",
        "\n",
        "APPLY_GAUSSIAN_BLUR   = True\n",
        "GAUSS_KSIZE           = 3\n",
        "APPLY_TOPHAT          = True\n",
        "TOPHAT_KSIZE          = 51\n",
        "\n",
        "THRESH_METHOD         = \"percent_of_max\"  # \"otsu\",\"adaptive\",\"percent_of_max\"\n",
        "PERCENT_OF_MAX        = 0.12\n",
        "\n",
        "\n",
        "MORPH_CLOSE           = True\n",
        "MORPH_CLOSE_KSIZE     = 5\n",
        "\n",
        "\n",
        "MIN_BLOB_AREA         = 60\n",
        "MIN_REL_POWER         = 0.03\n",
        "PERP_TOL_PX           = 14\n",
        "\n",
        "\n",
        "MAX_ABS_ORDER         = 2\n",
        "SYMMETRIZE_PM_PAIRS   = True\n",
        "ENFORCE_DECR_ABS_M    = True\n",
        "\n",
        "# Profiles & windows (for +1-only analysis)\n",
        "ANG_BINS              = 360\n",
        "PLUS1_BAND_FRAC       = 0.08   # annulus half-width as fraction of +1 radius (>=4 px)\n",
        "PLUS1_MIN_BAND_PX     = 4\n",
        "LINE_AVG_HALFWIDTH    = 2      # average ± this many pixels for line profiles\n",
        "LINE_ROI_HALFWIDTH    = 40     # show cross-sections within ± this px around +1 centroid\n",
        "\n",
        "SAVE_DIR              = \"oam_plots\"\n",
        "SAVE_SUMMARY_NAME     = \"summary_4plots.png\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "#processing functions\n",
        "def imread_gray01(filebytes):\n",
        "    arr = np.frombuffer(filebytes, np.uint8)\n",
        "    img = cv2.imdecode(arr, cv2.IMREAD_UNCHANGED)\n",
        "    if img is None:\n",
        "        raise ValueError(\"Could not decode image.\")\n",
        "    if img.ndim == 2:\n",
        "        g = img.astype(np.float32)\n",
        "    else:\n",
        "        if USE_RED_IF_COLOR:\n",
        "            g = img[:, :, 2].astype(np.float32)\n",
        "        else:\n",
        "            g = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY).astype(np.float32)\n",
        "    g -= g.min()\n",
        "    m = g.max()\n",
        "    if m > 0: g /= m\n",
        "    return g\n",
        "\n",
        "def center_crop_to_common_size(imgs):\n",
        "    if not imgs: return imgs\n",
        "    Hc = min(im.shape[0] for im in imgs)\n",
        "    Wc = min(im.shape[1] for im in imgs)\n",
        "    out = []\n",
        "    for im in imgs:\n",
        "        H, W = im.shape\n",
        "        y0 = (H - Hc)//2\n",
        "        x0 = (W - Wc)//2\n",
        "        out.append(im[y0:y0+Hc, x0:x0+Wc].copy())\n",
        "    return out\n",
        "\n",
        "def preprocess(g):\n",
        "    out = g.copy()\n",
        "    if APPLY_GAUSSIAN_BLUR and GAUSS_KSIZE % 2 == 1 and GAUSS_KSIZE >= 3:\n",
        "        out = cv2.GaussianBlur(out, (GAUSS_KSIZE, GAUSS_KSIZE), 0)\n",
        "    if APPLY_TOPHAT and TOPHAT_KSIZE % 2 == 1 and TOPHAT_KSIZE >= 3:\n",
        "        k = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (TOPHAT_KSIZE, TOPHAT_KSIZE))\n",
        "        out = cv2.morphologyEx((out * 255).astype(np.uint8), cv2.MORPH_TOPHAT, k).astype(np.float32) / 255.0\n",
        "    out -= out.min()\n",
        "    m = out.max()\n",
        "    if m > 0: out /= m\n",
        "    return out\n",
        "\n",
        "def combine(images, method=\"mean\"):\n",
        "    st = np.stack(images, 0)\n",
        "    return np.median(st, 0) if method == \"median\" else np.mean(st, 0)\n",
        "\n",
        "def find_center(img):\n",
        "    y0, x0 = np.unravel_index(np.argmax(img), img.shape)\n",
        "    win = 25\n",
        "    y1 = max(0, y0 - win); y2 = min(img.shape[0], y0 + win + 1)\n",
        "    x1 = max(0, x0 - win); x2 = min(img.shape[1], x0 + win + 1)\n",
        "    patch = img[y1:y2, x1:x2]\n",
        "    yy, xx = np.mgrid[y1:y2, x1:x2]\n",
        "    tot = patch.sum()\n",
        "    if tot <= 0: return float(y0), float(x0)\n",
        "    cy = (yy * patch).sum() / tot; cx = (xx * patch).sum() / tot\n",
        "    return float(cy), float(cx)\n",
        "\n",
        "def threshold_image(img):\n",
        "    g8 = (img * 255).astype(np.uint8)\n",
        "    if THRESH_METHOD == \"otsu\":\n",
        "        _, bw = cv2.threshold(g8, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "    elif THRESH_METHOD == \"adaptive\":\n",
        "        bw = cv2.adaptiveThreshold(g8, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
        "                                   cv2.THRESH_BINARY, 101, -5)\n",
        "    else:\n",
        "        thr = int(PERCENT_OF_MAX * 255)\n",
        "        _, bw = cv2.threshold(g8, thr, 255, cv2.THRESH_BINARY)\n",
        "    if MORPH_CLOSE and MORPH_CLOSE_KSIZE >= 3:\n",
        "        k = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (MORPH_CLOSE_KSIZE, MORPH_CLOSE_KSIZE))\n",
        "        bw = cv2.morphologyEx(bw, cv2.MORPH_CLOSE, k)\n",
        "    k3 = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
        "    bw = cv2.morphologyEx(bw, cv2.MORPH_OPEN, k3)\n",
        "    return bw\n",
        "\n",
        "def label_blobs(bw, min_area=50):\n",
        "    num, lab = cv2.connectedComponents(bw, 8)\n",
        "    blobs = []\n",
        "    for idx in range(1, num):\n",
        "        m = (lab == idx)\n",
        "        area = int(m.sum())\n",
        "        if area < min_area: continue\n",
        "        ys, xs = np.nonzero(m)\n",
        "        blobs.append({\n",
        "            \"mask\": m,\n",
        "            \"area\": area,\n",
        "            \"centroid\": (float(ys.mean()), float(xs.mean())),\n",
        "            \"sum_intensity\": 0.0\n",
        "        })\n",
        "    return blobs\n",
        "\n",
        "def assign_orders(blobs, center):\n",
        "    if not blobs: return {}, np.array([1.0, 0.0])\n",
        "    cy, cx = center\n",
        "    offs = np.array([[b[\"centroid\"][1] - cx, b[\"centroid\"][0] - cy] for b in blobs], float)\n",
        "    offs -= offs.mean(0, keepdims=True)\n",
        "    _, _, Vt = np.linalg.svd(offs, full_matrices=False)\n",
        "    axis = Vt[0]\n",
        "    proj = offs @ axis\n",
        "    dists = np.linalg.norm(offs, axis=1)\n",
        "    zero_idx = int(np.argmin(dists))\n",
        "    zpos = proj[zero_idx]\n",
        "    idx = np.argsort(proj)\n",
        "    right = [i for i in idx if proj[i] > zpos]\n",
        "    left  = [i for i in idx if proj[i] < zpos]\n",
        "    orders = {zero_idx: 0}\n",
        "    for k, i in enumerate(sorted(right, key=lambda i: abs(proj[i] - zpos)), 1): orders[i] = +k\n",
        "    for k, i in enumerate(sorted(left,  key=lambda i: abs(proj[i] - zpos)), 1): orders[i] = -k\n",
        "    return {i: orders.get(i) for i in range(len(blobs))}, axis\n",
        "\n",
        "def radial_profile_masked(img, center, mask):\n",
        "    H, W = img.shape; cy, cx = center\n",
        "    y, x = np.indices((H, W))\n",
        "    r = np.sqrt((x - cx)**2 + (y - cy)**2).astype(np.int32)\n",
        "    rr = r[mask]; vv = img[mask]\n",
        "    tbin = np.bincount(rr.ravel(), vv.ravel())\n",
        "    cnt  = np.bincount(rr.ravel())\n",
        "    return tbin / np.maximum(cnt, 1)\n",
        "\n",
        "def angular_profile(img, center, r_min, r_max, nbins=360):\n",
        "    H, W = img.shape; cy, cx = center\n",
        "    y, x = np.indices((H, W))\n",
        "    r  = np.sqrt((x - cx)**2 + (y - cy)**2)\n",
        "    th = np.arctan2(y - cy, x - cx)\n",
        "    mask = (r >= r_min) & (r <= r_max)\n",
        "    edges = np.linspace(-np.pi, np.pi, nbins + 1)\n",
        "    sums, _  = np.histogram(th[mask], bins=edges, weights=img[mask])\n",
        "    cnts, _  = np.histogram(th[mask], bins=edges)\n",
        "    prof = sums / np.maximum(cnts, 1)\n",
        "    centers_deg = 0.5 * (edges[:-1] + edges[1:]) * 180/np.pi\n",
        "    return centers_deg, prof\n",
        "\n",
        "# Algorithm\n",
        "print(\"Select one or more images (JPG/PNG) of the SAME pattern to combine:\")\n",
        "uploads = files.upload()\n",
        "if not uploads: raise SystemExit(\"No files uploaded.\")\n",
        "\n",
        "imgs = []\n",
        "for name, fb in uploads.items():\n",
        "    try:\n",
        "        g = imread_gray01(fb)\n",
        "        imgs.append(g)\n",
        "        print(f\"Loaded: {name}  shape={g.shape}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Skipping {name}: {e}\")\n",
        "if not imgs: raise SystemExit(\"No valid images.\")\n",
        "\n",
        "# unify sizes\n",
        "if CENTER_CROP_TO_COMMON:\n",
        "    imgs = center_crop_to_common_size(imgs)\n",
        "\n",
        "# preprocess + combine\n",
        "imgs = [preprocess(im) for im in imgs]\n",
        "combo = combine(imgs, method=AVERAGING_METHOD)\n",
        "print(f\"Combined {len(imgs)} image(s) by {AVERAGING_METHOD} averaging.\")\n",
        "\n",
        "# center\n",
        "center = find_center(combo)\n",
        "print(f\"Detected center (y, x) = ({center[0]:.1f}, {center[1]:.1f})\")\n",
        "\n",
        "# blobs\n",
        "bw    = threshold_image(combo)\n",
        "blobs = label_blobs(bw, min_area=MIN_BLOB_AREA)\n",
        "for b in blobs: b[\"sum_intensity\"] = combo[b[\"mask\"]].sum()\n",
        "\n",
        "# power floor\n",
        "max_pow = max([b[\"sum_intensity\"] for b in blobs], default=1.0)\n",
        "blobs   = [b for b in blobs if b[\"sum_intensity\"] >= MIN_REL_POWER * max_pow]\n",
        "\n",
        "# on-axis gate\n",
        "order_map, axis_vec = assign_orders(blobs, center)\n",
        "perp_vec = np.array([-axis_vec[1], axis_vec[0]])\n",
        "def perp_dist(b):\n",
        "    cy, cx = center\n",
        "    off = np.array([b[\"centroid\"][1] - cx, b[\"centroid\"][0] - cy])\n",
        "    return abs(off @ perp_vec)\n",
        "blobs = [b for b in blobs if perp_dist(b) <= PERP_TOL_PX]\n",
        "\n",
        "#order assignment\n",
        "order_map, axis_vec = assign_orders(blobs, center)\n",
        "\n",
        "#Modes regularization\n",
        "order_power = defaultdict(float)\n",
        "for i, b in enumerate(blobs):\n",
        "    m = order_map.get(i, None)\n",
        "    if m is None: continue\n",
        "    if MAX_ABS_ORDER is not None and abs(m) > MAX_ABS_ORDER: continue\n",
        "    order_power[m] += float(b[\"sum_intensity\"])\n",
        "\n",
        "\n",
        "for m in range(-2, 3):\n",
        "    order_power.setdefault(m, 0.0)\n",
        "\n",
        "def _warn(msg):\n",
        "    print(\"[Modes check]\", msg)\n",
        "\n",
        "# Diagnostic\n",
        "if abs(order_power[+2] - order_power[-2]) > 0.05 * max(1.0, order_power[0]):\n",
        "    _warn(f\"±2 asymmetry detected: +2={order_power[+2]:.3g}, -2={order_power[-2]:.3g}\")\n",
        "if abs(order_power[+1] - order_power[-1]) > 0.05 * max(1.0, order_power[0]):\n",
        "    _warn(f\"±1 asymmetry detected: +1={order_power[+1]:.3g}, -1={order_power[-1]:.3g}\")\n",
        "if (order_power[+2] > order_power[+1]) or (order_power[-2] > order_power[-1]):\n",
        "    _warn(\"|m|=2 appears stronger than |m|=1 (before adjustment).\")\n",
        "\n",
        "\n",
        "if SYMMETRIZE_PM_PAIRS:\n",
        "    p1 = 0.5 * (order_power[+1] + order_power[-1])\n",
        "    p2 = 0.5 * (order_power[+2] + order_power[-2])\n",
        "    order_power[+1] = order_power[-1] = p1\n",
        "    order_power[+2] = order_power[-2] = p2\n",
        "\n",
        "\n",
        "if ENFORCE_DECR_ABS_M:\n",
        "    p1 = max(order_power[+1], order_power[-1])\n",
        "    order_power[+2] = min(order_power[+2], p1)\n",
        "    order_power[-2] = min(order_power[-2], p1)\n",
        "\n",
        "\n",
        "if abs(order_power[+2] - order_power[-2]) > 1e-12:\n",
        "    _warn(\"±2 still asymmetric after symmetrization (numerical)\")\n",
        "if (order_power[+2] > order_power[+1]) or (order_power[-2] > order_power[-1]):\n",
        "    _warn(\"|m|=2 still stronger than |m|=1 after regularization.\")\n",
        "\n",
        "orders_range  = list(range(-2, 3))\n",
        "powers_sorted = [order_power[o] for o in orders_range]\n",
        "\n",
        "#locate +1 spot\n",
        "plus1_radii = []\n",
        "plus1_centroid = None\n",
        "for i, b in enumerate(blobs):\n",
        "    m = order_map.get(i, None)\n",
        "    if m == +1:\n",
        "        by, bx = b[\"centroid\"]; plus1_centroid = (by, bx)\n",
        "        plus1_radii.append(np.hypot(bx - center[1], by - center[0]))\n",
        "if not plus1_radii:\n",
        "    for i, b in enumerate(blobs):\n",
        "        m = order_map.get(i, None)\n",
        "        if abs(m) == 1:\n",
        "            by, bx = b[\"centroid\"]\n",
        "            plus1_radii.append(np.hypot(bx - center[1], by - center[0]))\n",
        "if not plus1_radii:\n",
        "    raise SystemExit(\"Could not locate the +1 ring. Try lowering threshold or filters.\")\n",
        "\n",
        "R1   = float(np.median(plus1_radii))\n",
        "band = max(PLUS1_MIN_BAND_PX, PLUS1_BAND_FRAC * R1)\n",
        "r1_min = max(0.0, R1 - band)\n",
        "r1_max = R1 + band\n",
        "\n",
        "H, W = combo.shape\n",
        "yy, xx = np.indices((H, W))\n",
        "rr = np.sqrt((xx - center[1])**2 + (yy - center[0])**2)\n",
        "mask_plus1 = (rr >= r1_min) & (rr <= r1_max)\n",
        "\n",
        "#figure\n",
        "fig = plt.figure(figsize=(14, 10))\n",
        "outer = fig.add_gridspec(2, 2, wspace=0.3, hspace=0.3)\n",
        "ax_mode    = fig.add_subplot(outer[0, 0])\n",
        "ax_radial  = fig.add_subplot(outer[0, 1])\n",
        "ax_angular = fig.add_subplot(outer[1, 0])\n",
        "inner      = outer[1, 1].subgridspec(1, 2, wspace=0.3)\n",
        "ax_h       = fig.add_subplot(inner[0, 0])\n",
        "ax_v       = fig.add_subplot(inner[0, 1])\n",
        "\n",
        "# 1) Modes intensity (-2..+2)\n",
        "ax_mode.bar(orders_range, powers_sorted)\n",
        "ax_mode.set_xlabel(\"Diffraction order (m)\")\n",
        "ax_mode.set_ylabel(\"Integrated power (a.u.)\")\n",
        "ax_mode.set_title(\"Modes Intensity (−2 … +2)\")\n",
        "\n",
        "# 2) Radial intensity (ONLY +1 annulus)\n",
        "rad_plus1 = radial_profile_masked(combo, center, mask_plus1)\n",
        "ax_radial.plot(np.arange(len(rad_plus1)), rad_plus1)\n",
        "ax_radial.set_xlabel(\"Radius (pixels) from center\")\n",
        "ax_radial.set_ylabel(\"Average intensity (a.u.)\")\n",
        "ax_radial.set_title(\"Radial Intensity (on +1 annulus)\")\n",
        "\n",
        "# 3) Angular intensity (ONLY +1 annulus)\n",
        "ang_deg, ang_val = angular_profile(combo, center, r1_min, r1_max, nbins=ANG_BINS)\n",
        "ax_angular.plot(ang_val, ang_deg)\n",
        "ax_angular.set_ylabel(\"Azimuthal angle (degrees)\")\n",
        "ax_angular.set_xlabel(\"Average intensity (a.u.)\")\n",
        "ax_angular.set_title(\"Angular Intensity (on +1 annulus)\")\n",
        "\n",
        "# 4) Line cross-sections THROUGH +1 (local window)\n",
        "if plus1_centroid is None:\n",
        "    plus1_centroid = (center[0], center[1])\n",
        "py, px = int(round(plus1_centroid[0])), int(round(plus1_centroid[1]))\n",
        "y1 = max(0, py - LINE_AVG_HALFWIDTH); y2 = min(H, py + LINE_AVG_HALFWIDTH + 1)\n",
        "x1 = max(0, px - LINE_AVG_HALFWIDTH); x2 = min(W, px + LINE_AVG_HALFWIDTH + 1)\n",
        "\n",
        "horiz = combo[y1:y2, :].mean(axis=0)\n",
        "xh0 = max(0, px - LINE_ROI_HALFWIDTH); xh1 = min(W, px + LINE_ROI_HALFWIDTH + 1)\n",
        "ax_h.plot(np.arange(xh0, xh1) - px, horiz[xh0:xh1])\n",
        "ax_h.set_title(\"Horizontal cross-section +1\")\n",
        "ax_h.set_xlabel(\"x (pixels relative to +1 centroid)\")\n",
        "ax_h.set_ylabel(\"Intensity (a.u.)\")\n",
        "\n",
        "vert = combo[:, x1:x2].mean(axis=1)\n",
        "yv0 = max(0, py - LINE_ROI_HALFWIDTH); yv1 = min(H, py + LINE_ROI_HALFWIDTH + 1)\n",
        "ax_v.plot(vert[yv0:yv1], np.arange(yv0, yv1) - py)\n",
        "ax_v.set_title(\"Vertical cross-section +1\")\n",
        "ax_v.set_xlabel(\"Intensity (a.u.)\")\n",
        "ax_v.set_ylabel(\"y (pixels relative to +1 centroid)\")\n",
        "\n",
        "fig.tight_layout()\n",
        "fig.savefig(os.path.join(SAVE_DIR, SAVE_SUMMARY_NAME), dpi=220)\n",
        "plt.show()\n",
        "print(\"Saved:\", os.path.join(SAVE_DIR, SAVE_SUMMARY_NAME))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#OAM Annular Grating analysis\n",
        "#For future work, when enough images have been collected for annular grating fringe pattern from OAM = -2 to +2, we can train the small dataset using resnet\n",
        "#A sample CNN based code has been given below\n",
        "!pip -q install opencv-python-headless numpy matplotlib scipy\n",
        "import numpy as np, cv2, matplotlib.pyplot as plt, os, csv\n",
        "from google.colab import files\n",
        "from scipy.signal import find_peaks, savgol_filter\n",
        "from numpy.fft import rfft\n",
        "\n",
        "#parameters\n",
        "# Annulus center (ring) selection\n",
        "R_SELECT_MODE          = \"largest_prominence\"\n",
        "R_K_INDEX              = 1\n",
        "R_SCALE                = 1.65\n",
        "\n",
        "\n",
        "ANNULUS_HALF_FRAC      = 0.22                   # half-width as fraction of R\n",
        "ANNULUS_MIN_HALFW_PX   = 10\n",
        "\n",
        "# Robustness: suppress angular outliers (side streaks)\n",
        "OUTLIER_SIGMA_CLIP     = 2.5\n",
        "\n",
        "# Preprocessing\n",
        "USE_RED_IF_COLOR       = True\n",
        "CENTER_CROP_TO_COMMON  = True\n",
        "AVERAGING_METHOD       = \"mean\"                 # \"mean\" or \"median\"\n",
        "APPLY_GAUSS            = True; GAUSS_KSIZE = 3\n",
        "APPLY_TOPHAT           = True; TOPHAT_KSIZE = 51\n",
        "\n",
        "# Radial peak find (for R_raw)\n",
        "RAD_PEAK_PROM_REL      = 0.05\n",
        "RAD_PEAK_MIN_DIST_PX   = 6\n",
        "\n",
        "# Angular profile binning & smoothing\n",
        "ANG_BINS               = 720\n",
        "BASELINE_REMOVE        = True\n",
        "BASELINE_MOVING_WIN    = 61\n",
        "SMOOTH_METHOD          = \"savgol\"\n",
        "SAVGOL_WIN             = 15\n",
        "SAVGOL_POLY            = 2\n",
        "MOVING_WIN             = 9\n",
        "\n",
        "# Harmonic (FFT) inference\n",
        "KMAX_HARMONIC          = 6\n",
        "HARMONIC_RATIO_MIN     = 1.25\n",
        "\n",
        "\n",
        "PEAK_PROM_REL          = 0.15\n",
        "PEAK_MIN_DIST_DEG      = 30\n",
        "\n",
        "OUT_DIR                = \"oam_donut_out\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "def imread_gray01(filebytes):\n",
        "    arr = np.frombuffer(filebytes, np.uint8)\n",
        "    img = cv2.imdecode(arr, cv2.IMREAD_UNCHANGED)\n",
        "    if img is None: raise ValueError(\"Could not decode image.\")\n",
        "    if img.ndim == 2:\n",
        "        g = img.astype(np.float32)\n",
        "    else:\n",
        "        g = img[:,:,2].astype(np.float32) if USE_RED_IF_COLOR \\\n",
        "            else cv2.cvtColor(img, cv2.COLOR_BGR2GRAY).astype(np.float32)\n",
        "    g -= g.min(); M = g.max()\n",
        "    if M>0: g/=M\n",
        "    return g\n",
        "\n",
        "def center_crop_to_common(imgs):\n",
        "    Hc = min(im.shape[0] for im in imgs)\n",
        "    Wc = min(im.shape[1] for im in imgs)\n",
        "    out=[]\n",
        "    for im in imgs:\n",
        "        H,W = im.shape\n",
        "        y0=(H-Hc)//2; x0=(W-Wc)//2\n",
        "        out.append(im[y0:y0+Hc, x0:x0+Wc].copy())\n",
        "    return out\n",
        "\n",
        "def preprocess(g):\n",
        "    out = g.copy()\n",
        "    if APPLY_GAUSS and GAUSS_KSIZE%2==1 and GAUSS_KSIZE>=3:\n",
        "        out = cv2.GaussianBlur(out, (GAUSS_KSIZE,GAUSS_KSIZE), 0)\n",
        "    if APPLY_TOPHAT and TOPHAT_KSIZE%2==1 and TOPHAT_KSIZE>=3:\n",
        "        k = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (TOPHAT_KSIZE,TOPHAT_KSIZE))\n",
        "        out = cv2.morphologyEx((out*255).astype(np.uint8), cv2.MORPH_TOPHAT, k).astype(np.float32)/255.0\n",
        "    out -= out.min(); M = out.max()\n",
        "    if M>0: out/=M\n",
        "    return out\n",
        "\n",
        "def combine(imgs, method=\"mean\"):\n",
        "    st = np.stack(imgs,0)\n",
        "    return np.median(st,0) if method==\"median\" else np.mean(st,0)\n",
        "\n",
        "def find_center(img):\n",
        "    y0,x0 = np.unravel_index(np.argmax(img), img.shape)\n",
        "    win=25\n",
        "    y1=max(0,y0-win); y2=min(img.shape[0],y0+win+1)\n",
        "    x1=max(0,x0-win); x2=min(img.shape[1],x0+win+1)\n",
        "    patch = img[y1:y2, x1:x2]; yy,xx=np.mgrid[y1:y2, x1:x2]\n",
        "    s = patch.sum()\n",
        "    if s<=0: return float(y0), float(x0)\n",
        "    cy=(yy*patch).sum()/s; cx=(xx*patch).sum()/s\n",
        "    return float(cy), float(cx)\n",
        "\n",
        "def radial_profile(img, center):\n",
        "    H,W = img.shape; cy,cx=center\n",
        "    y,x = np.indices((H,W))\n",
        "    r = np.sqrt((x-cx)**2 + (y-cy)**2)\n",
        "    r_i = r.astype(np.int32)\n",
        "    tbin = np.bincount(r_i.ravel(), img.ravel())\n",
        "    cnt  = np.bincount(r_i.ravel())\n",
        "    return tbin/np.maximum(cnt,1), r\n",
        "\n",
        "def moving_average(y, k):\n",
        "    if k<=1 or k%2==0: return y\n",
        "    pad=k//2\n",
        "    ypad=np.pad(y,(pad,pad),mode='edge')\n",
        "    kern=np.ones(k)/k\n",
        "    return np.convolve(ypad,kern,mode='valid')\n",
        "\n",
        "def mad_z(x):\n",
        "    med = np.median(x)\n",
        "    mad = np.median(np.abs(x - med)) + 1e-9\n",
        "    return 0.6745 * (x - med) / mad\n",
        "\n",
        "print(\"Upload one or more images (same donut):\")\n",
        "uploads = files.upload()\n",
        "if not uploads: raise SystemExit(\"No files uploaded.\")\n",
        "imgs=[]\n",
        "for n,fb in uploads.items():\n",
        "    try:\n",
        "        g = imread_gray01(fb); imgs.append(g)\n",
        "        print(\"Loaded:\", n, g.shape)\n",
        "    except Exception as e:\n",
        "        print(\"Skipping\", n, e)\n",
        "if not imgs: raise SystemExit(\"No valid images.\")\n",
        "if CENTER_CROP_TO_COMMON: imgs = center_crop_to_common(imgs)\n",
        "imgs = [preprocess(im) for im in imgs]\n",
        "combo = combine(imgs, method=AVERAGING_METHOD)\n",
        "H,W = combo.shape\n",
        "\n",
        "center = find_center(combo)\n",
        "rad_prof, rmap = radial_profile(combo, center)\n",
        "r_pix = np.arange(len(rad_prof))\n",
        "\n",
        "r_ignore = max(3, int(0.01*min(H,W)))\n",
        "r_work = r_pix[r_ignore:]; p_work = rad_prof[r_ignore:]\n",
        "p_s = savgol_filter(p_work, 11, 2) if len(p_work)>=11 else p_work\n",
        "prom = RAD_PEAK_PROM_REL * (np.max(p_s) if p_s.size else 1.0)\n",
        "pk_all, props = find_peaks(p_s, prominence=prom, distance=RAD_PEAK_MIN_DIST_PX)\n",
        "if pk_all.size == 0:\n",
        "    raise SystemExit(\"No radial peaks found. Try larger TOPHAT_KSIZE or smaller RAD_PEAK_PROM_REL.\")\n",
        "\n",
        "if R_SELECT_MODE == \"first\":\n",
        "    R_raw = float(r_work[pk_all[0]])\n",
        "elif R_SELECT_MODE == \"largest_prominence\":\n",
        "    idx = int(np.argmax(props[\"prominences\"]))\n",
        "    R_raw = float(r_work[pk_all[idx]])\n",
        "elif R_SELECT_MODE == \"kth\":\n",
        "    k = int(np.clip(R_K_INDEX, 0, len(pk_all)-1))\n",
        "    R_raw = float(r_work[pk_all[k]])\n",
        "else:\n",
        "    R_raw = float(r_work[pk_all[0]])\n",
        "\n",
        "cy,cx = center\n",
        "max_r_possible = np.sqrt(max(cy, H-cy-1)**2 + max(cx, W-cx-1)**2) - 2.0\n",
        "R = min(R_SCALE * R_raw, max_r_possible)\n",
        "\n",
        "halfw = max(ANNULUS_MIN_HALFW_PX, ANNULUS_HALF_FRAC * R)\n",
        "rmin, rmax = max(0.0, R - halfw), min(max_r_possible, R + halfw)\n",
        "\n",
        "# graph\n",
        "yy,xx = np.indices(combo.shape)\n",
        "rr = np.sqrt((xx-cx)**2 + (yy-cy)**2)\n",
        "theta = np.arctan2(yy-cy, xx-cx)\n",
        "mask = (rr>=rmin) & (rr<=rmax)\n",
        "\n",
        "edges = np.linspace(-np.pi, np.pi, ANG_BINS+1)\n",
        "vals, _ = np.histogram(theta[mask], bins=edges, weights=combo[mask])\n",
        "cnts, _ = np.histogram(theta[mask], bins=edges)\n",
        "ang = vals/np.maximum(cnts,1)\n",
        "ang_deg = 0.5*(edges[:-1]+edges[1:]) * 180/np.pi\n",
        "\n",
        "# outlier clipping\n",
        "z = mad_z(ang)\n",
        "ang_clipped = ang.copy()\n",
        "ang_clipped[np.abs(z) > OUTLIER_SIGMA_CLIP] = np.median(ang)\n",
        "\n",
        "if BASELINE_REMOVE and len(ang_clipped)>=BASELINE_MOVING_WIN:\n",
        "    base = moving_average(ang_clipped, BASELINE_MOVING_WIN)\n",
        "    ang_d = ang_clipped - base\n",
        "else:\n",
        "    ang_d = ang_clipped - np.mean(ang_clipped)\n",
        "\n",
        "if SMOOTH_METHOD==\"savgol\" and len(ang_d)>=SAVGOL_WIN and SAVGOL_WIN%2==1:\n",
        "    ang_s = savgol_filter(ang_d, SAVGOL_WIN, SAVGOL_POLY)\n",
        "elif SMOOTH_METHOD==\"moving\" and len(ang_d)>=MOVING_WIN and MOVING_WIN%2==1:\n",
        "    ang_s = moving_average(ang_d, MOVING_WIN)\n",
        "else:\n",
        "    ang_s = ang_d\n",
        "\n",
        "A = np.abs(rfft(ang_s))\n",
        "A = A / (A.max() + 1e-12)\n",
        "harm_k = np.arange(len(A))\n",
        "valid = (harm_k >= 1) & (harm_k <= KMAX_HARMONIC)\n",
        "k_vals = harm_k[valid]\n",
        "amps  = A[valid]\n",
        "\n",
        "order = int(k_vals[np.argmax(amps)])\n",
        "sorted_idx = np.argsort(amps)[::-1]\n",
        "dom, second = amps[sorted_idx[0]], amps[sorted_idx[1]] if len(sorted_idx)>1 else 0.0\n",
        "confident = (dom >= HARMONIC_RATIO_MIN * max(second, 1e-9))\n",
        "\n",
        "#visualization\n",
        "from scipy.signal import find_peaks\n",
        "promA = 0.15 * (np.max(ang_s) - np.min(ang_s) + 1e-9)\n",
        "min_dist_bins = max(1, int(PEAK_MIN_DIST_DEG * ANG_BINS / 360.0))\n",
        "pk_idx, _ = find_peaks(ang_s, prominence=promA, distance=min_dist_bins)\n",
        "\n",
        "#figure\n",
        "overlay = cv2.cvtColor((combo*255).astype(np.uint8), cv2.COLOR_GRAY2BGR)\n",
        "cv2.circle(overlay, (int(round(cx)), int(round(cy))), int(round(R_raw)), (0,255,0), 1)\n",
        "cv2.circle(overlay, (int(round(cx)), int(round(cy))), int(round(R)), (0,200,0), 2)\n",
        "cv2.circle(overlay, (int(round(cx)), int(round(cy))), int(round(rmin)), (255,0,0), 1)\n",
        "cv2.circle(overlay, (int(round(cx)), int(round(cy))), int(round(rmax)), (255,0,0), 1)\n",
        "\n",
        "fig = plt.figure(figsize=(13,9))\n",
        "gs  = fig.add_gridspec(2,2, wspace=0.28, hspace=0.28)\n",
        "\n",
        "ax0 = fig.add_subplot(gs[0,0])\n",
        "ax0.imshow(overlay[...,::-1])\n",
        "ax0.set_title(f\"Annulus: R_raw={R_raw:.1f}px  →  R_scaled={R:.1f}px  |  halfw={halfw:.1f}px\")\n",
        "ax0.axis(\"off\")\n",
        "\n",
        "ax1 = fig.add_subplot(gs[0,1])\n",
        "ax1.plot(ang,      ang_deg, lw=0.9, alpha=0.35, label=\"raw\")\n",
        "ax1.plot(ang_s,    ang_deg, lw=1.4, label=\"smoothed/detrended & clipped\")\n",
        "ax1.plot(ang_s[pk_idx], ang_deg[pk_idx], 'ro', ms=4, label=\"peaks (viz)\")\n",
        "ax1.set_xlabel(\"Average intensity (a.u.)\")\n",
        "ax1.set_ylabel(\"Azimuthal angle (deg)\")\n",
        "ax1.set_title(\"Angular intensity on annulus\")\n",
        "ax1.legend(loc=\"lower right\")\n",
        "\n",
        "ax2 = fig.add_subplot(gs[1,0])\n",
        "ax2.plot(r_pix, rad_prof, label=\"radial mean\")\n",
        "ax2.axvline(R_raw, color='g', ls='--', lw=1, label=\"R_raw (peak)\")\n",
        "ax2.axvline(R,     color='g', ls='-',  lw=1.2, label=\"R_scaled\")\n",
        "ax2.fill_betweenx([0, rad_prof.max()], rmin, rmax, color='tab:blue', alpha=0.12, label=\"annulus\")\n",
        "ax2.set_xlabel(\"Radius from donut center (pixels)\")\n",
        "ax2.set_ylabel(\"Mean intensity (a.u.)\")\n",
        "ax2.set_title(\"Radial profile & chosen annulus\")\n",
        "ax2.legend()\n",
        "\n",
        "ax3 = fig.add_subplot(gs[1,1])\n",
        "ax3.bar(k_vals, amps, width=0.8)\n",
        "ax3.set_xticks(k_vals)\n",
        "ax3.set_xlabel(\"Harmonic index k\")\n",
        "ax3.set_ylabel(\"Normalized amplitude\")\n",
        "ax3.set_title(f\"Harmonic spectrum of angular profile → inferred |ℓ| = {order}\"\n",
        "              + (\"\" if confident else \" (low confidence)\"))\n",
        "\n",
        "fig.tight_layout()\n",
        "png_path = os.path.join(OUT_DIR, \"donut_harmonics_summary.png\")\n",
        "fig.savefig(png_path, dpi=220)\n",
        "plt.show()\n",
        "\n",
        "csv_path = os.path.join(OUT_DIR, \"harmonics_table.csv\")\n",
        "with open(csv_path, \"w\", newline=\"\") as f:\n",
        "    w = csv.writer(f)\n",
        "    w.writerow([\"k\", \"norm_amplitude\"])\n",
        "    for k,a in zip(k_vals, amps):\n",
        "        w.writerow([int(k), float(a)])\n",
        "print(f\"Inferred |ell| = {order}  (dominant={dom:.3f}, next={second:.3f}, confident={confident})\")\n",
        "print(\"Saved figure:\", png_path)\n",
        "print(\"Saved spectrum CSV:\", csv_path)"
      ],
      "metadata": {
        "id": "E8BvLWGH0cpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CNN Approach to Annular Grating dark fringe analysis using pre trained\n",
        "#with the help of CNN there is no need to manually tweak the model as we did in the previous codes and that makes it more generalized and accurate in classification\n",
        "#Once the dataset is large enough by collecting enough sample images of dark fringes, we can train the model from scratch for customization\n",
        "# a reference paper talks about using CNN classification approach in free space for OAM based communication - Ye, J., Kang, H., Wang, H., Shen, C., Jahannia, B., Heidari, E., Asadizanjani, N., Miri, M.A., Sorger, V.J. and Dalir, H., 2023, September. Demultiplexing oam beams via fourier optical convolutional neural network. In Laser Beam Shaping XXIII (Vol. 12667, pp. 16-33). SPIE.\n",
        "!pip -q install torch torchvision pandas pillow\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from torchvision import transforms, models\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch, torch.nn as nn\n",
        "from google.colab import files\n",
        "import io\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Custom dataset for OAM images\n",
        "class OAMDataset(Dataset):\n",
        "    def __init__(self, csv_file, transform=None):\n",
        "        self.data = pd.read_csv(csv_file)  # CSV with columns [image_path, label]\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.data.iloc[idx, 0]\n",
        "        label = int(self.data.iloc[idx, 1])\n",
        "        # Convert label to 0..4 for 5 classes\n",
        "        label = label + 2\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# Define image transformations (resize, normalize, etc.)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "#dataset upload .csv file\n",
        "print(\"Please upload your CSV file (with columns X=images, Y=labels):\")\n",
        "uploaded = files.upload()\n",
        "csv_path = list(uploaded.keys())[0]\n",
        "print(f\"Loaded dataset CSV: {csv_path}\")\n",
        "\n",
        "#training testing split (validation is 20 percent), can be used in the epoch loop to see training vs validation accuracy\n",
        "df = pd.read_csv(csv_path)\n",
        "dataset = OAMDataset(csv_path, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
        "train_df, val_df = train_test_split(df, test_size=0.2, stratify=df['Y'], random_state=42)\n",
        "train_dataset = OAMDataset(train_df, transform=transform)\n",
        "val_dataset   = OAMDataset(val_df, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "print(f\"Train size: {len(train_dataset)}, Validation size: {len(val_dataset)}\")\n",
        "\n",
        "# Load a pre-trained ResNet18 model\n",
        "model = models.resnet18(pretrained=True)\n",
        "num_features = model.fc.in_features\n",
        "model.fc = nn.Linear(num_features, 5)  # 5 output classes (-2,-1,0,1,2)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Define loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Training loop (outline)\n",
        "num_epochs = 5\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    for images, labels in dataloader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model.state_dict(), 'oam_cnn_model.pth')\n",
        "\n",
        "# Inference on a new image\n",
        "from torchvision import transforms\n",
        "\n",
        "# Load the trained model weights\n",
        "model = models.resnet18(pretrained=False)\n",
        "model.fc = nn.Linear(num_features, 5)\n",
        "model.load_state_dict(torch.load('oam_cnn_model.pth'))\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Prepare the image\n",
        "img_path = 'new_oam_image.jpg'\n",
        "image = Image.open(img_path).convert('RGB')\n",
        "img_tensor = transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "# Predict OAM class\n",
        "with torch.no_grad():\n",
        "    output = model(img_tensor)\n",
        "    _, pred = torch.max(output, 1)\n",
        "oam_pred = pred.item() - 2\n",
        "print(\"Predicted OAM value:\", oam_pred)\n"
      ],
      "metadata": {
        "id": "jYvePgDp10El"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}